{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Setup + Document Loading\n",
    "\n",
    "**Goal**: Load DataFlow's enterprise documents for RAG system\n",
    "\n",
    "\n",
    "## Requirements\n",
    "```\n",
    "langchain==0.1.0\n",
    "langchain-community==0.0.13\n",
    "faiss-cpu==1.7.4\n",
    "sentence-transformers==2.2.2\n",
    "pandas==2.0.3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:09:56.976016Z",
     "start_time": "2025-09-23T14:09:56.958261Z"
    }
   },
   "source": [
    "# Imports and setup\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain.document_loaders import TextLoader, CSVLoader, JSONLoader, UnstructuredMarkdownLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Path to enterprise documents\n",
    "KNOWLEDGE_BASE_PATH = Path(\"./enterprise_knowledge_base\")\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Documents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:11:34.154438Z",
     "start_time": "2025-09-23T14:11:32.982751Z"
    }
   },
   "source": [
    "def load_enterprise_documents(base_path: Path) -> List[Document]:\n",
    "    \"\"\"Load all documents recursively with proper metadata\"\"\"\n",
    "    \n",
    "    all_docs = []\n",
    "    \n",
    "    print(\" Loading DataFlow's documents...\")\n",
    "    \n",
    "    # Process each department folder\n",
    "    for dept_path in base_path.iterdir():\n",
    "        if not dept_path.is_dir():\n",
    "            continue\n",
    "            \n",
    "        department = dept_path.name\n",
    "        print(f\" {department}...\")\n",
    "        \n",
    "        # Get ALL files recursively\n",
    "        files = [f for f in dept_path.rglob(\"*\") if f.is_file()]\n",
    "        \n",
    "        for file_path in files:\n",
    "            try:\n",
    "                # Choose loader by extension\n",
    "                ext = file_path.suffix.lower()\n",
    "                if ext == '.csv':\n",
    "                    loader = CSVLoader(str(file_path))\n",
    "                elif ext == '.json':\n",
    "                    loader = JSONLoader(str(file_path), jq_schema='.', text_content=False)\n",
    "                elif ext == '.md':\n",
    "                    loader = UnstructuredMarkdownLoader(str(file_path))\n",
    "                else:\n",
    "                    loader = TextLoader(str(file_path), encoding='utf-8')\n",
    "                \n",
    "                # Load and add metadata\n",
    "                docs = loader.load()\n",
    "                for doc in docs:\n",
    "                    doc.metadata.update({\n",
    "                        \"department\": department,\n",
    "                        \"source_file\": file_path.name,\n",
    "                        \"file_type\": ext\n",
    "                    })\n",
    "                \n",
    "                all_docs.extend(docs)\n",
    "                rel_path = file_path.relative_to(dept_path)\n",
    "                print(f\"    {rel_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   {file_path.name}: {str(e)[:30]}...\")\n",
    "    \n",
    "    # Quick summary\n",
    "    departments = set(doc.metadata['department'] for doc in all_docs)\n",
    "    total_chars = sum(len(doc.page_content) for doc in all_docs)\n",
    "    \n",
    "    print(f\"\\n LOADED: {len(all_docs)} documents from {len(departments)} departments\")\n",
    "    print(f\"Content: {total_chars:,} characters\")\n",
    "    print(f\" Departments: {', '.join(sorted(departments))}\")\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "# Load all documents\n",
    "documents = load_enterprise_documents(KNOWLEDGE_BASE_PATH)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading DataFlow's documents...\n",
      " business_data...\n",
      "    billing_and_pricing.csv\n",
      "    customer_analytics.csv\n",
      "    integration_partners.csv\n",
      " customer_facing...\n",
      "   api_documentation.json: jq package not found, please i...\n",
      "    competitive_analysis.txt\n",
      "    product_user_guide.markdown\n",
      "    terms_of_service.markdown\n",
      "    troubleshooting_guide.txt\n",
      " internal_operations...\n",
      "    hr_policies/employee_handbook.txt\n",
      "   onboarding_checklist.json: jq package not found, please i...\n",
      "   release_notes.json: jq package not found, please i...\n",
      "   sales_playbook.json: jq package not found, please i...\n",
      "    support_operations/customer_support_procedures.markdown\n",
      "    support_operations/system_architecture.markdown\n",
      " legal_compliance...\n",
      "    compliance_certifications.csv\n",
      "    privacy_policy.txt\n",
      "    security_policies.txt\n",
      "    terms_of_service.markdown\n",
      "\n",
      " LOADED: 208 documents from 4 departments\n",
      "Content: 204,565 characters\n",
      " Departments: business_data, customer_facing, internal_operations, legal_compliance\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:12:00.144568Z",
     "start_time": "2025-09-23T14:12:00.131593Z"
    }
   },
   "source": [
    "# Quick validation\n",
    "print(\"ðŸ” Validation:\")\n",
    "print(f\"   Documents: {len(documents)} (target: 20+)\")\n",
    "print(f\"   Departments: {len(set(doc.metadata['department'] for doc in documents))} (target: 4)\")\n",
    "print(f\"   Content: {sum(len(doc.page_content) for doc in documents):,} chars (target: 10,000+)\")\n",
    "\n",
    "if len(documents) >= 15:\n",
    "    print(\"\\n SUCCESS! Ready for Part 2: Text Chunking\")\n",
    "else:\n",
    "    print(\"\\n Low document count - check folder structure\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Validation:\n",
      "   Documents: 208 (target: 20+)\n",
      "   Departments: 4 (target: 4)\n",
      "   Content: 204,565 chars (target: 10,000+)\n",
      "\n",
      " SUCCESS! Ready for Part 2: Text Chunking\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Text Chunking\n",
    "\n",
    "**Goal**: Transform 212 documents into optimally-sized chunks for RAG\n",
    "**Why Critical**: Bad chunking = bad RAG responses. Good chunking = accurate answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:12:43.749366Z",
     "start_time": "2025-09-23T14:12:43.619195Z"
    }
   },
   "source": [
    "# Imports for text chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "print(\"Text chunking tools imported!\")\n",
    "print(f\" Starting with {len(documents)} documents from Part 1\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text chunking tools imported!\n",
      " Starting with 208 documents from Part 1\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Chunking Strategy\n",
    "\n",
    "**Industry Best Practice**: 1000 characters with 200 overlap\n",
    "- **Why 1000 chars?** Perfect balance for embedding models\n",
    "- **Why 200 overlap?** Preserves context across chunks\n",
    "- **Recursive splitting**: Tries sentences, then words, then characters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:27:38.805931Z",
     "start_time": "2025-09-23T14:27:38.747007Z"
    }
   },
   "source": [
    "def create_smart_chunks(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Split documents into optimal chunks for RAG\"\"\"\n",
    "    \n",
    "    print(\" Creating smart chunks...\")\n",
    "    \n",
    "    # Industry-standard chunking settings\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        # Optimal for embedding models\n",
    "        chunk_overlap=200,      # Preserve context\n",
    "        length_function=len,    # Character-based\n",
    "        separators=[            # Try these in order:\n",
    "            \"\\n\\n\",              # Paragraphs first\n",
    "            \"\\n\",                # Then lines\n",
    "            \". \",                # Then sentences\n",
    "            \" \",                 # Then words\n",
    "            \"\",                  # Finally characters\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    stats = {\n",
    "        \"original_docs\": len(documents),\n",
    "        \"total_chunks\": 0,\n",
    "        \"by_department\": {},\n",
    "        \"by_file_type\": {}\n",
    "    }\n",
    "    \n",
    "    # Process each department\n",
    "    for dept in set(doc.metadata['department'] for doc in documents):\n",
    "        dept_docs = [doc for doc in documents if doc.metadata['department'] == dept]\n",
    "        dept_chunks = []\n",
    "        \n",
    "        print(f\" {dept}: {len(dept_docs)} docs â†’ \", end=\"\")\n",
    "        \n",
    "        for doc in dept_docs:\n",
    "            # Split the document\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            \n",
    "            # Add chunk metadata\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk.metadata.update({\n",
    "                    \"chunk_id\": f\"{doc.metadata['source_file']}_{i}\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"chunk_size\": len(chunk.page_content)\n",
    "                })\n",
    "            \n",
    "            dept_chunks.extend(chunks)\n",
    "            \n",
    "            # Track stats\n",
    "            file_type = doc.metadata.get('file_type', 'unknown')\n",
    "            stats[\"by_file_type\"][file_type] = stats[\"by_file_type\"].get(file_type, 0) + len(chunks)\n",
    "        \n",
    "        stats[\"by_department\"][dept] = len(dept_chunks)\n",
    "        stats[\"total_chunks\"] += len(dept_chunks)\n",
    "        all_chunks.extend(dept_chunks)\n",
    "        \n",
    "        print(f\"{len(dept_chunks)} chunks\")\n",
    "    \n",
    "    print(f\"\\n CHUNKING COMPLETE:\")\n",
    "    print(f\"    Original: {stats['original_docs']} documents\")\n",
    "    print(f\"    Created: {stats['total_chunks']} chunks\")\n",
    "    print(f\"    Ratio: {stats['total_chunks'] / stats['original_docs']:.1f} chunks per document\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Create chunks\n",
    "chunks = create_smart_chunks(documents)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating smart chunks...\n",
      " legal_compliance: 28 docs â†’ 80 chunks\n",
      " business_data: 173 docs â†’ 173 chunks\n",
      " internal_operations: 3 docs â†’ 57 chunks\n",
      " customer_facing: 4 docs â†’ 82 chunks\n",
      "\n",
      " CHUNKING COMPLETE:\n",
      "    Original: 208 documents\n",
      "    Created: 392 chunks\n",
      "    Ratio: 1.9 chunks per document\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Chunk Quality"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:28:51.366134Z",
     "start_time": "2025-09-23T14:28:51.350874Z"
    }
   },
   "source": [
    "def analyze_chunk_quality(chunks: List[Document]):\n",
    "    \"\"\"Analyze chunk distribution and quality\"\"\"\n",
    "    \n",
    "    print(\" CHUNK QUALITY ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Size analysis\n",
    "    sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "    avg_size = sum(sizes) / len(sizes)\n",
    "    min_size = min(sizes)\n",
    "    max_size = max(sizes)\n",
    "    \n",
    "    print(f\"Size Distribution:\")\n",
    "    print(f\"   Average: {avg_size:.0f} characters\")\n",
    "    print(f\"   Range: {min_size} - {max_size} characters\")\n",
    "    \n",
    "    # Size buckets\n",
    "    buckets = {\n",
    "        \"Small (0-500)\": sum(1 for s in sizes if s <= 500),\n",
    "        \"Medium (500-1000)\": sum(1 for s in sizes if 500 < s <= 1000),\n",
    "        \"Large (1000+)\": sum(1 for s in sizes if s > 1000)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n Size Distribution:\")\n",
    "    for bucket, count in buckets.items():\n",
    "        percentage = (count / len(chunks)) * 100\n",
    "        print(f\"   {bucket}: {count} chunks ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Department distribution\n",
    "    by_dept = {}\n",
    "    for chunk in chunks:\n",
    "        dept = chunk.metadata['department']\n",
    "        by_dept[dept] = by_dept.get(dept, 0) + 1\n",
    "    \n",
    "    print(f\"\\n By Department:\")\n",
    "    for dept, count in sorted(by_dept.items()):\n",
    "        percentage = (count / len(chunks)) * 100\n",
    "        print(f\"   {dept}: {count} chunks ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    optimal_chunks = sum(1 for s in sizes if 500 <= s <= 1000)\n",
    "    quality_score = (optimal_chunks / len(chunks)) * 100\n",
    "    \n",
    "    print(f\"\\n Quality Score: {quality_score:.1f}%\")\n",
    "    print(f\"   ({optimal_chunks}/{len(chunks)} chunks in optimal range)\")\n",
    "    \n",
    "    if quality_score >= 70:\n",
    "        print(\" Excellent chunking quality!\")\n",
    "    elif quality_score >= 50:\n",
    "        print(\" Good chunking quality\")\n",
    "    else:\n",
    "        print(\"ï¸ Consider adjusting chunk size\")\n",
    "\n",
    "analyze_chunk_quality(chunks)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHUNK QUALITY ANALYSIS\n",
      "------------------------------\n",
      "Size Distribution:\n",
      "   Average: 548 characters\n",
      "   Range: 3 - 998 characters\n",
      "\n",
      " Size Distribution:\n",
      "   Small (0-500): 209 chunks (53.3%)\n",
      "   Medium (500-1000): 183 chunks (46.7%)\n",
      "   Large (1000+): 0 chunks (0.0%)\n",
      "\n",
      " By Department:\n",
      "   business_data: 173 chunks (44.1%)\n",
      "   customer_facing: 82 chunks (20.9%)\n",
      "   internal_operations: 57 chunks (14.5%)\n",
      "   legal_compliance: 80 chunks (20.4%)\n",
      "\n",
      " Quality Score: 46.7%\n",
      "   (183/392 chunks in optimal range)\n",
      "ï¸ Consider adjusting chunk size\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Chunks Review"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:29:03.534013Z",
     "start_time": "2025-09-23T14:29:03.514288Z"
    }
   },
   "source": [
    "# Show sample chunks from different departments\n",
    "print(\" SAMPLE CHUNKS REVIEW\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "departments = list(set(chunk.metadata['department'] for chunk in chunks))\n",
    "\n",
    "for dept in departments[:3]:  # Show first 3 departments\n",
    "    dept_chunks = [c for c in chunks if c.metadata['department'] == dept]\n",
    "    if dept_chunks:\n",
    "        sample = dept_chunks[0]  # First chunk from department\n",
    "        \n",
    "        print(f\"\\n{dept.upper()}:\")\n",
    "        print(f\"   File: {sample.metadata['source_file']}\")\n",
    "        print(f\"   Size: {len(sample.page_content)} chars\")\n",
    "        print(f\"   Preview: {sample.page_content[:150]}...\")\n",
    "        print(f\"   Metadata: {sample.metadata['chunk_id']}\")\n",
    "\n",
    "print(f\"\\n Total chunks ready for vector storage: {len(chunks)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SAMPLE CHUNKS REVIEW\n",
      "-------------------------\n",
      "\n",
      "BUSINESS_DATA:\n",
      "   File: billing_and_pricing.csv\n",
      "   Size: 299 chars\n",
      "   Preview: Plan_Type: Users\n",
      "Feature: Number of users\n",
      "Starter_Plan: 5\n",
      "Professional_Plan: 25\n",
      "Enterprise_Plan: Unlimited\n",
      "Notes: Additional users: $10/user/month (Pr...\n",
      "   Metadata: billing_and_pricing.csv_0\n",
      "\n",
      "INTERNAL_OPERATIONS:\n",
      "   File: employee_handbook.txt\n",
      "   Size: 664 chars\n",
      "   Preview: DataFlow Solutions Employee Handbook\n",
      "Last Updated: June 8, 2025\n",
      "\n",
      "Welcome to DataFlow Solutions! This handbook outlines our policies, procedures, and c...\n",
      "   Metadata: employee_handbook.txt_0\n",
      "\n",
      "CUSTOMER_FACING:\n",
      "   File: competitive_analysis.txt\n",
      "   Size: 877 chars\n",
      "   Preview: DataFlow Solutions Competitive Analysis\n",
      "Last Updated: June 8, 2025\n",
      "\n",
      "This document analyzes DataFlow Solutionsâ€™ position in the SaaS BI and data visual...\n",
      "   Metadata: competitive_analysis.txt_0\n",
      "\n",
      " Total chunks ready for vector storage: 392\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:29:58.517969Z",
     "start_time": "2025-09-23T14:29:58.502746Z"
    }
   },
   "source": [
    "# Validate chunking success\n",
    "print(\"ðŸ” CHUNKING VALIDATION\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Basic checks\n",
    "total_original_chars = sum(len(doc.page_content) for doc in documents)\n",
    "total_chunk_chars = sum(len(chunk.page_content) for chunk in chunks)\n",
    "content_preserved = (total_chunk_chars / total_original_chars) * 100\n",
    "\n",
    "checks = [\n",
    "    (len(chunks) > len(documents), f\"More chunks than docs: {len(chunks)} > {len(documents)}\"),\n",
    "    (content_preserved >= 90, f\"Content preserved: {content_preserved:.1f}%\"),\n",
    "    (all('chunk_id' in c.metadata for c in chunks), \"All chunks have IDs\"),\n",
    "    (all('department' in c.metadata for c in chunks), \"All chunks have departments\")\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for passed, message in checks:\n",
    "    status = \"pass\" if passed else \"fail\"\n",
    "    print(f\"   {status} {message}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\nSUCCESS! Chunks ready for Part 3: Vector Embeddings\")\n",
    "else:\n",
    "    print(\"\\n Some validation checks failed\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” CHUNKING VALIDATION\n",
      "--------------------\n",
      "   pass More chunks than docs: 392 > 208\n",
      "   pass Content preserved: 105.1%\n",
      "   pass All chunks have IDs\n",
      "   pass All chunks have departments\n",
      "\n",
      "SUCCESS! Chunks ready for Part 3: Vector Embeddings\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Vector Embeddings & Search\n",
    "\n",
    "**Goal**: Transform text chunks into searchable mathematical vectors\n",
    "**Why Critical**: This enables semantic search - finding meaning, not just keywords\n",
    "\n",
    "\n",
    "Then run: `pip install sentence-transformers==4.1.0 huggingface-hub==0.32.4 langchain-huggingface`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:34:07.636250Z",
     "start_time": "2025-09-23T14:31:22.700274Z"
    }
   },
   "source": [
    "# Modern imports (no deprecation warnings)\n",
    "try:\n",
    "    # Modern approach - no deprecation warnings\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    print(\" Using modern langchain-huggingface (recommended)\")\n",
    "    modern_import = True\n",
    "except ImportError:\n",
    "    # Fallback to deprecated version if needed\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    print(\" Using deprecated import (consider upgrading)\")\n",
    "    print(\" Run: pip install langchain-huggingface\")\n",
    "    modern_import = False\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "print(\" Vector tools imported!\")\n",
    "print(f\"Ready to embed {len(chunks)} chunks from Part 2\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using modern langchain-huggingface (recommended)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theshika/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vector tools imported!\n",
      "Ready to embed 392 chunks from Part 2\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embedding Model\n",
    "\n",
    "**Model Choice**: `all-MiniLM-L6-v2`\n",
    "- **Fast**: Perfect for development and production\n",
    "- **Accurate**: Great semantic understanding\n",
    "- **Compact**: 384 dimensions (vs 1536 for OpenAI)\n",
    "- **Free**: No API costs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:39:24.470719Z",
     "start_time": "2025-09-23T14:35:00.620303Z"
    }
   },
   "source": [
    "def setup_embedding_model():\n",
    "    \"\"\"Initialize the embedding model for vector creation\"\"\"\n",
    "    \n",
    "    print(\"Loading embedding model...\")\n",
    "    \n",
    "    # Use production-grade embedding model\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    # Modern LangChain wrapper (no deprecation warnings)\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={'device': 'cpu'},  # Use CPU for compatibility\n",
    "        encode_kwargs={'normalize_embeddings': True}  # Better for similarity search\n",
    "    )\n",
    "    \n",
    "    print(f\"Model loaded: {model_name}\")\n",
    "    print(f\"Vector dimensions: 384\")\n",
    "    print(f\"Device: CPU (production compatible)\")\n",
    "    \n",
    "    if modern_import:\n",
    "        print(\" Using modern non-deprecated embeddings!\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Setup embeddings\n",
    "embeddings = setup_embedding_model()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Model loaded: sentence-transformers/all-MiniLM-L6-v2\n",
      "Vector dimensions: 384\n",
      "Device: CPU (production compatible)\n",
      " Using modern non-deprecated embeddings!\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Store\n",
    "\n",
    "**FAISS**: Facebook's vector search library\n",
    "- Powers Instagram recommendations\n",
    "- Billion-scale vector search\n",
    "- Lightning-fast similarity search\n",
    "- Industry standard for RAG systems"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:40:03.259021Z",
     "start_time": "2025-09-23T14:39:41.716720Z"
    }
   },
   "source": [
    "def create_vector_store(chunks: List, embeddings) -> FAISS:\n",
    "    \"\"\"Create FAISS vector store from text chunks\"\"\"\n",
    "    \n",
    "    print(\" Creating vector embeddings...\")\n",
    "    print(\" This may take 30-60 seconds...\")\n",
    "    \n",
    "    # Create vector store with FAISS\n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    print(f\" Vector store created!\")\n",
    "    print(f\" Vectors: {len(chunks)}\")\n",
    "    print(f\" Dimensions: 384 per vector\")\n",
    "    print(f\"Total size: ~{len(chunks) * 384 * 4 / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = create_vector_store(chunks, embeddings)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating vector embeddings...\n",
      " This may take 30-60 seconds...\n",
      " Vector store created!\n",
      " Vectors: 392\n",
      " Dimensions: 384 per vector\n",
      "Total size: ~0.6 MB\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:41:57.082197Z",
     "start_time": "2025-09-23T14:41:56.898494Z"
    }
   },
   "source": [
    "def test_semantic_search(vector_store: FAISS, test_queries: List[str]):\n",
    "    \"\"\"Test the vector store with realistic customer queries\"\"\"\n",
    "    \n",
    "    print(\" TESTING SEMANTIC SEARCH\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n Query {i}: '{query}'\")\n",
    "        \n",
    "        # Search for most relevant chunks\n",
    "        results = vector_store.similarity_search(\n",
    "            query=query, \n",
    "            k=3  # Top 3 most relevant chunks\n",
    "        )\n",
    "        \n",
    "        print(f\" Found {len(results)} relevant chunks:\")\n",
    "        \n",
    "        for j, result in enumerate(results, 1):\n",
    "            dept = result.metadata['department']\n",
    "            file = result.metadata['source_file']\n",
    "            preview = result.page_content[:100].replace('\\n', ' ')\n",
    "            \n",
    "            print(f\"   {j}.  {dept} |  {file}\")\n",
    "            print(f\"      Preview: {preview}...\")\n",
    "\n",
    "# Test with realistic customer queries\n",
    "test_queries = [\n",
    "    \"What are your pricing plans?\",\n",
    "    \"How do I integrate with your API?\",\n",
    "    \"What is your privacy policy?\",\n",
    "    \"I'm having trouble with authentication\",\n",
    "    \"Employee handbook and HR policies\"\n",
    "]\n",
    "\n",
    "test_semantic_search(vector_store, test_queries)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TESTING SEMANTIC SEARCH\n",
      "------------------------------\n",
      "\n",
      " Query 1: 'What are your pricing plans?'\n",
      " Found 3 relevant chunks:\n",
      "   1.  business_data |  billing_and_pricing.csv\n",
      "      Preview: Plan_Type: Pricing Feature: Base price (USD Starter_Plan: annual) Professional_Plan: $470/year Enter...\n",
      "   2.  business_data |  billing_and_pricing.csv\n",
      "      Preview: Plan_Type: Add-Ons Feature: Priority data processing Starter_Plan: No Professional_Plan: $200/month ...\n",
      "   3.  business_data |  billing_and_pricing.csv\n",
      "      Preview: Plan_Type: Pricing Feature: Base price (GBP Starter_Plan: annual) Professional_Plan: Â£385/year Enter...\n",
      "\n",
      " Query 2: 'How do I integrate with your API?'\n",
      " Found 3 relevant chunks:\n",
      "   1.  customer_facing |  product_user_guide.markdown\n",
      "      Preview: ### 10.4 References - API Documentation: `api_documentation.json` - Troubleshooting Guide: `troubles...\n",
      "   2.  internal_operations |  system_architecture.markdown\n",
      "      Preview: ---  ## 3. API Gateway and Microservices The platform uses an API Gateway and microservices architec...\n",
      "   3.  business_data |  billing_and_pricing.csv\n",
      "      Preview: Plan_Type: API Calls Feature: API endpoints Starter_Plan: Basic Professional_Plan: Full Enterprise_P...\n",
      "\n",
      " Query 3: 'What is your privacy policy?'\n",
      " Found 3 relevant chunks:\n",
      "   1.  internal_operations |  employee_handbook.txt\n",
      "      Preview: 2.2 Anti-Harassment We maintain a zero-tolerance policy for harassment, including verbal, physical, ...\n",
      "   2.  legal_compliance |  privacy_policy.txt\n",
      "      Preview: DataFlow Solutions Privacy Policy Last Updated: June 8, 2025  This Privacy Policy outlines how DataF...\n",
      "   3.  legal_compliance |  terms_of_service.markdown\n",
      "      Preview: This agreement incorporates our Privacy Policy (privacy_policy.txt), Security Policies (security_pol...\n",
      "\n",
      " Query 4: 'I'm having trouble with authentication'\n",
      " Found 3 relevant chunks:\n",
      "   1.  customer_facing |  troubleshooting_guide.txt\n",
      "      Preview: ---  7. API Authentication Failure Error Code: API-4001 Error Message: \"401: Unauthorized\" Descripti...\n",
      "   2.  legal_compliance |  security_policies.txt\n",
      "      Preview: 1.2 2FA Policy - Mandatory for all employees and customers (product_user_guide.md, Section 2.2). - U...\n",
      "   3.  customer_facing |  troubleshooting_guide.txt\n",
      "      Preview: ---  16. SSO Login Failure Error Code: SSO-9001 Error Message: \"SSO login failed: Invalid configurat...\n",
      "\n",
      " Query 5: 'Employee handbook and HR policies'\n",
      " Found 3 relevant chunks:\n",
      "   1.  internal_operations |  employee_handbook.txt\n",
      "      Preview: ---  3. Remote Work and Flexible Schedules DataFlow supports hybrid and remote work for 80% of emplo...\n",
      "   2.  legal_compliance |  privacy_policy.txt\n",
      "      Preview: 8.2 Organizational Measures - Employee training: Annual GDPR/CCPA training (employee_handbook.txt, S...\n",
      "   3.  legal_compliance |  security_policies.txt\n",
      "      Preview: ---  References - Employee Handbook: employee_handbook.txt - Compliance Certifications: compliance_c...\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:43:42.093240Z",
     "start_time": "2025-09-23T14:43:41.951733Z"
    }
   },
   "source": [
    "def analyze_search_quality(vector_store: FAISS):\n",
    "    \"\"\"Analyze the quality and coverage of semantic search\"\"\"\n",
    "    \n",
    "    print(\" SEARCH QUALITY ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Test queries for each department\n",
    "    dept_queries = {\n",
    "        \"business_data\": \"pricing and billing information\",\n",
    "        \"customer_facing\": \"product features and user guide\",\n",
    "        \"internal_operations\": \"employee policies and procedures\", \n",
    "        \"legal_compliance\": \"privacy and terms of service\"\n",
    "    }\n",
    "    \n",
    "    coverage_score = 0\n",
    "    total_tests = len(dept_queries)\n",
    "    \n",
    "    for dept, query in dept_queries.items():\n",
    "        print(f\"\\n Testing {dept} coverage...\")\n",
    "        \n",
    "        results = vector_store.similarity_search(query, k=5)\n",
    "        \n",
    "        # Check if top results are from the right department\n",
    "        dept_matches = sum(1 for r in results if r.metadata['department'] == dept)\n",
    "        accuracy = (dept_matches / len(results)) * 100 if results else 0\n",
    "        \n",
    "        print(f\"   Query: '{query}'\")\n",
    "        print(f\"   Accuracy: {dept_matches}/{len(results)} = {accuracy:.1f}%\")\n",
    "        \n",
    "        if accuracy >= 60:  # At least 3/5 results from correct dept\n",
    "            coverage_score += 1\n",
    "            print(f\"   Good coverage\")\n",
    "        else:\n",
    "            print(f\"    Needs improvement\")\n",
    "    \n",
    "    overall_score = (coverage_score / total_tests) * 100\n",
    "    \n",
    "    print(f\"\\n OVERALL SEARCH QUALITY: {overall_score:.1f}%\")\n",
    "    print(f\"   ({coverage_score}/{total_tests} departments with good coverage)\")\n",
    "    \n",
    "    if overall_score >= 75:\n",
    "        print(\" Excellent search quality!\")\n",
    "    elif overall_score >= 50:\n",
    "        print(\" Good search quality\")\n",
    "    else:\n",
    "        print(\" Consider more diverse chunks or better embeddings\")\n",
    "    \n",
    "    return overall_score\n",
    "\n",
    "quality_score = analyze_search_quality(vector_store)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SEARCH QUALITY ANALYSIS\n",
      "------------------------------\n",
      "\n",
      " Testing business_data coverage...\n",
      "   Query: 'pricing and billing information'\n",
      "   Accuracy: 3/5 = 60.0%\n",
      "   Good coverage\n",
      "\n",
      " Testing customer_facing coverage...\n",
      "   Query: 'product features and user guide'\n",
      "   Accuracy: 0/5 = 0.0%\n",
      "    Needs improvement\n",
      "\n",
      " Testing internal_operations coverage...\n",
      "   Query: 'employee policies and procedures'\n",
      "   Accuracy: 2/5 = 40.0%\n",
      "    Needs improvement\n",
      "\n",
      " Testing legal_compliance coverage...\n",
      "   Query: 'privacy and terms of service'\n",
      "   Accuracy: 4/5 = 80.0%\n",
      "   Good coverage\n",
      "\n",
      " OVERALL SEARCH QUALITY: 50.0%\n",
      "   (2/4 departments with good coverage)\n",
      " Good search quality\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:45:30.785814Z",
     "start_time": "2025-09-23T14:45:30.388600Z"
    }
   },
   "source": [
    "# Save vector store for production use\n",
    "def save_vector_store(vector_store: FAISS, save_path: str = \"dataflow_vector_store\"):\n",
    "    \"\"\"Save vector store to disk for reuse\"\"\"\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saving vector store to '{save_path}'...\")\n",
    "    \n",
    "    try:\n",
    "        vector_store.save_local(save_path)\n",
    "        print(f\"Vector store saved successfully!\")\n",
    "        print(f\"Location: {save_path}/\")\n",
    "        print(f\" Can be loaded later with: FAISS.load_local('{save_path}', embeddings)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Save failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Save the vector store\n",
    "save_success = save_vector_store(vector_store)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving vector store to 'dataflow_vector_store'...\n",
      "Vector store saved successfully!\n",
      "Location: dataflow_vector_store/\n",
      " Can be loaded later with: FAISS.load_local('dataflow_vector_store', embeddings)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:46:18.135675Z",
     "start_time": "2025-09-23T14:46:18.068587Z"
    }
   },
   "source": [
    "# Final validation\n",
    "print(\" VECTOR STORE VALIDATION\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Test basic functionality\n",
    "test_query = \"pricing information\"\n",
    "test_results = vector_store.similarity_search(test_query, k=1)\n",
    "\n",
    "checks = [\n",
    "    (len(test_results) > 0, \"Vector search returns results\"),\n",
    "    (hasattr(vector_store, 'index'), \"FAISS index created\"),\n",
    "    (save_success, \"Vector store saved successfully\"),\n",
    "    (len(chunks) > 0, f\"All {len(chunks)} chunks embedded\"),\n",
    "    (modern_import, \"Using modern non-deprecated imports\")\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for passed, message in checks:\n",
    "    status = \"pass\" if passed else \"fail\"\n",
    "    print(f\"   {status} {message}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\n SUCCESS! Vector store ready for Part 4: RAG Agent\")\n",
    "    print(\" You now have a production-grade semantic search system!\")\n",
    "    print(f\" Search quality: {quality_score:.1f}% accuracy\")\n",
    "else:\n",
    "    print(\"\\nï¸ Some validation checks failed\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VECTOR STORE VALIDATION\n",
      "-------------------------\n",
      "   pass Vector search returns results\n",
      "   pass FAISS index created\n",
      "   pass Vector store saved successfully\n",
      "   pass All 392 chunks embedded\n",
      "   pass Using modern non-deprecated imports\n",
      "\n",
      " SUCCESS! Vector store ready for Part 4: RAG Agent\n",
      " You now have a production-grade semantic search system!\n",
      " Search quality: 50.0% accuracy\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: RAG Agent - Complete Intelligent System\n",
    "\n",
    "**Goal**: Connect vector search to LLM for intelligent customer service\n",
    "\n",
    "\n",
    "\n",
    "## LLM Setup Required\n",
    "**Ollama (Free, Local)**\n",
    "```bash\n",
    "# Install Ollama from https://ollama.ai\n",
    "ollama pull llama3.2  # Download model\n",
    "ollama serve         # Start server\n",
    "```\n",
    "\n",
    "**Alternative: OpenAI (if you prefer)**\n",
    "```bash\n",
    "pip install openai\n",
    "# Set OPENAI_API_KEY environment variable\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T14:46:51.476267Z",
     "start_time": "2025-09-23T14:46:47.825121Z"
    }
   },
   "source": [
    "# Imports for RAG agent\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import time\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Simple Ollama setup\n",
    "llm = None\n",
    "\n",
    "try:\n",
    "    from langchain.llms import Ollama\n",
    "    llm = Ollama(model=\"llama3.2\", base_url=\"http://localhost:11434\")\n",
    "    # Test connection\n",
    "    test_response = llm.invoke(\"Hello\")\n",
    "    print(\"âœ… Ollama LLM connected successfully!\")\n",
    "    print(\"ðŸ†“ Using free local LLM\")\n",
    "    print(f\"ðŸ“Š Vector store ready: {len(chunks)} chunks\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ollama connection failed: {e}\")\n",
    "    print(\"ðŸ’¡ Make sure Ollama is running: ollama serve\")\n",
    "    print(\"ðŸ’¡ And model is downloaded: ollama pull llama3.2\")\n",
    "    llm = None"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_tracing_context' from 'langsmith' (/home/theshika/.local/lib/python3.10/site-packages/langsmith/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Imports for RAG agent\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RetrievalQA\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprompts\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PromptTemplate\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmemory\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ConversationBufferMemory\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/__init__.py:20\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"**Chains** are easily reusable components linked together.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mChains encode a sequence of calls to components like models, document retrievers,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m    Chain --> <name>Chain  # Examples: LLMChain, MapReduceChain, RouterChain\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m APIChain\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mopenapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchain\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m OpenAPIEndpointChain\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcombine_documents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AnalyzeDocumentChain\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/api/base.py:12\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprompts\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BasePromptTemplate\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpydantic_v1\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Field, root_validator\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmanager\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     13\u001B[0m     AsyncCallbackManagerForChainRun,\n\u001B[1;32m     14\u001B[0m     CallbackManagerForChainRun,\n\u001B[1;32m     15\u001B[0m )\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprompt\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m API_RESPONSE_PROMPT, API_URL_PROMPT\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Chain\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/callbacks/__init__.py:17\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LangChainDeprecationWarning\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     14\u001B[0m     StdOutCallbackHandler,\n\u001B[1;32m     15\u001B[0m     StreamingStdOutCallbackHandler,\n\u001B[1;32m     16\u001B[0m )\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtracers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontext\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     18\u001B[0m     collect_runs,\n\u001B[1;32m     19\u001B[0m     tracing_enabled,\n\u001B[1;32m     20\u001B[0m     tracing_v2_enabled,\n\u001B[1;32m     21\u001B[0m )\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtracers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LangChainTracer\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfile\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FileCallbackHandler\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/langchain_core/tracers/context.py:20\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangsmith\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m run_helpers \u001B[38;5;28;01mas\u001B[39;00m ls_rh\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangsmith\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m utils \u001B[38;5;28;01mas\u001B[39;00m ls_utils\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtracers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LangChainTracer\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtracers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrun_collector\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RunCollectorCallbackHandler\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/langchain_core/tracers/langchain.py:11\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, Any, Optional, Union\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01muuid\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m UUID\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangsmith\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Client, get_tracing_context\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangsmith\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m run_trees \u001B[38;5;28;01mas\u001B[39;00m rt\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangsmith\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m utils \u001B[38;5;28;01mas\u001B[39;00m ls_utils\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'get_tracing_context' from 'langsmith' (/home/theshika/.local/lib/python3.10/site-packages/langsmith/__init__.py)"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Customer Service Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Professional customer service prompt created\n",
      "ðŸŽ¯ Optimized for helpful, accurate responses\n"
     ]
    }
   ],
   "source": [
    "# Professional customer service prompt\n",
    "CUSTOMER_SERVICE_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are DataFlow's helpful customer service assistant. Your job is to provide accurate, friendly, and professional support to customers.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use the provided context to answer questions accurately\n",
    "- Be concise but thorough in your explanations\n",
    "- If information isn't in the context, say \"I don't have that specific information\" and suggest contacting support\n",
    "- Always maintain a helpful and professional tone\n",
    "- For technical questions, provide step-by-step guidance when possible\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "CUSTOMER QUESTION:\n",
    "{question}\n",
    "\n",
    "RESPONSE:\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Professional customer service prompt created\")\n",
    "print(\"ðŸŽ¯ Optimized for helpful, accurate responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Creating RAG chain...\n",
      "âœ… RAG chain created successfully!\n",
      "ðŸ” Retriever: Top 4 most relevant chunks\n",
      "ðŸ¤– LLM: Ready for customer questions\n",
      "ðŸ“š Source attribution: Enabled\n"
     ]
    }
   ],
   "source": [
    "def create_rag_chain(vector_store, llm, prompt_template):\n",
    "    \"\"\"Create production RAG chain\"\"\"\n",
    "    \n",
    "    if not llm:\n",
    "        print(\"âŒ No LLM available - cannot create RAG chain\")\n",
    "        print(\"ðŸ’¡ Please install Ollama or set up OpenAI API key\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ðŸ”— Creating RAG chain...\")\n",
    "    \n",
    "    # Create retrieval QA chain\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # Stuff all context into prompt\n",
    "        retriever=vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}  # Retrieve top 4 most relevant chunks\n",
    "        ),\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": prompt_template\n",
    "        },\n",
    "        return_source_documents=True  # Show which documents were used\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… RAG chain created successfully!\")\n",
    "    print(\"ðŸ” Retriever: Top 4 most relevant chunks\")\n",
    "    print(\"ðŸ¤– LLM: Ready for customer questions\")\n",
    "    print(\"ðŸ“š Source attribution: Enabled\")\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = create_rag_chain(vector_store, llm, CUSTOMER_SERVICE_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Service Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– DataFlow Customer Service Agent initialized\n",
      "âœ… Customer service agent ready!\n"
     ]
    }
   ],
   "source": [
    "class DataFlowCustomerAgent:\n",
    "    \"\"\"Professional customer service agent with simple conversation tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_chain):\n",
    "        self.rag_chain = rag_chain\n",
    "        # Simple conversation tracking (no deprecated memory)\n",
    "        self.conversation_history = []\n",
    "        self.conversation_count = 0\n",
    "        self.response_times = []\n",
    "        \n",
    "        print(\"ðŸ¤– DataFlow Customer Service Agent initialized\")\n",
    "    \n",
    "    def ask(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Ask the agent a question and get a comprehensive response\"\"\"\n",
    "        \n",
    "        if not self.rag_chain:\n",
    "            return {\n",
    "                \"answer\": \"I'm sorry, but I'm not properly configured right now. Please contact our support team directly.\",\n",
    "                \"sources\": [],\n",
    "                \"response_time\": 0,\n",
    "                \"error\": \"No LLM available\"\n",
    "            }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Get response from RAG chain\n",
    "            response = self.rag_chain.invoke({\"query\": question})\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            \n",
    "            # Simple conversation tracking\n",
    "            self.conversation_history.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": response[\"result\"],\n",
    "                \"timestamp\": start_time\n",
    "            })\n",
    "            \n",
    "            # Track metrics\n",
    "            self.conversation_count += 1\n",
    "            self.response_times.append(response_time)\n",
    "            \n",
    "            # Extract source information\n",
    "            sources = []\n",
    "            if \"source_documents\" in response:\n",
    "                for doc in response[\"source_documents\"]:\n",
    "                    sources.append({\n",
    "                        \"department\": doc.metadata.get(\"department\", \"unknown\"),\n",
    "                        \"file\": doc.metadata.get(\"source_file\", \"unknown\"),\n",
    "                        \"preview\": doc.page_content[:100] + \"...\"\n",
    "                    })\n",
    "            \n",
    "            return {\n",
    "                \"answer\": response[\"result\"],\n",
    "                \"sources\": sources,\n",
    "                \"response_time\": response_time,\n",
    "                \"conversation_turn\": self.conversation_count\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"I apologize, but I encountered an error. Please try rephrasing or contact support.\",\n",
    "                \"sources\": [],\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get agent performance statistics\"\"\"\n",
    "        \n",
    "        if not self.response_times:\n",
    "            return {\"conversations\": 0, \"avg_response_time\": 0}\n",
    "        \n",
    "        return {\n",
    "            \"conversations\": self.conversation_count,\n",
    "            \"avg_response_time\": sum(self.response_times) / len(self.response_times),\n",
    "            \"fastest_response\": min(self.response_times),\n",
    "            \"slowest_response\": max(self.response_times),\n",
    "            \"total_history\": len(self.conversation_history)\n",
    "        }\n",
    "    \n",
    "    def get_conversation_history(self, last_n: int = 5):\n",
    "        \"\"\"Get recent conversation history\"\"\"\n",
    "        return self.conversation_history[-last_n:] if self.conversation_history else []\n",
    "\n",
    "# Create the customer service agent\n",
    "agent = DataFlowCustomerAgent(rag_chain)\n",
    "print(\"âœ… Customer service agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Customer Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ­ TESTING CUSTOMER SERVICE SCENARIOS\n",
      "=============================================\n",
      "\n",
      "ðŸ“ž Scenario 1: Billing\n",
      "â“ Question: What are your pricing plans and how much does the premium plan cost?\n",
      "--------------------------------------------------\n",
      "ðŸ¤– Agent Response:\n",
      "   Hello! I'm happy to help you with your question about our pricing plans.\n",
      "\n",
      "We have three main pricing plans:\n",
      "\n",
      "1. **Professional Plan**: This plan costs $500/month and includes priority escalation, dedicated CSM, and faster data processing.\n",
      "2. **Enterprise Plan**: This plan costs $1000/month and also includes the features from the Professional Plan, as well as additional benefits tailored to large enterprises.\n",
      "\n",
      "Additionally, we offer a **Base Price** option, which is our annual pricing model. The Base Price for each plan is:\n",
      "\n",
      "* Starter Plan: $470/year\n",
      "* Professional Plan: $470/year\n",
      "* Enterprise Plan: $1430/year\n",
      "\n",
      "Please note that these prices are subject to change, and you can always check our website or contact us for the most up-to-date information.\n",
      "\n",
      "If you have any further questions or would like more details on our pricing plans, feel free to ask!\n",
      "\n",
      "ðŸ“š Sources Used:\n",
      "   1. ðŸ“ business_data - billing_and_pricing.csv\n",
      "   2. ðŸ“ business_data - billing_and_pricing.csv\n",
      "   3. ðŸ“ business_data - billing_and_pricing.csv\n",
      "\n",
      "â±ï¸ Response Time: 31.11 seconds\n",
      "ðŸŽ¯ Department Accuracy: âœ… Accurate\n",
      "\n",
      "ðŸ“ž Scenario 2: Technical Support\n",
      "â“ Question: How do I authenticate with your API? I'm getting authentication errors.\n",
      "--------------------------------------------------\n",
      "ðŸ¤– Agent Response:\n",
      "   I'd be happy to help you with authenticating with our API!\n",
      "\n",
      "To get started, it looks like the issue is related to invalid or expired credentials. Can you please check if your API key is valid and not expired in Settings (product_user_guide.md, Section 2.3)?\n",
      "\n",
      "If your API key is correct, I recommend refreshing your OAuth token via /auth/token (api_documentation.json, OAuth 2.0). This should help resolve the authentication issue.\n",
      "\n",
      "To do this, please follow these steps:\n",
      "\n",
      "1. Open a new request in your preferred tool (e.g., cURL, Postman).\n",
      "2. Set the method to `GET` and the path to `/auth/token`.\n",
      "3. Include an `Authorization` header with your OAuth 2.0 token.\n",
      "4. Send the request and verify if you receive a valid response.\n",
      "\n",
      "If refreshing the OAuth token doesn't work, please ensure that the scopes are correct (e.g., dashboards:write; api_documentation.json). Additionally, double-check that the X-API-Key header is in the correct format (X-API-Key: your_key).\n",
      "\n",
      "If none of these steps resolve the issue, I recommend contacting our support team via P1 ticket to further assist you. We'll be happy to help you troubleshoot and find a solution.\n",
      "\n",
      "Please let me know if you have any questions or need further clarification on any of these steps!\n",
      "\n",
      "ðŸ“š Sources Used:\n",
      "   1. ðŸ“ customer_facing - troubleshooting_guide.txt\n",
      "   2. ðŸ“ customer_facing - api_documentation.json\n",
      "   3. ðŸ“ customer_facing - api_documentation.json\n",
      "\n",
      "â±ï¸ Response Time: 59.85 seconds\n",
      "ðŸŽ¯ Department Accuracy: âœ… Accurate\n",
      "\n",
      "ðŸ“ž Scenario 3: Privacy/Legal\n",
      "â“ Question: What data do you collect and how do you protect my privacy?\n",
      "--------------------------------------------------\n",
      "ðŸ¤– Agent Response:\n",
      "   Thank you for reaching out to DataFlow's customer support! We're committed to protecting your personal data and ensuring its confidentiality.\n",
      "\n",
      "We collect various types of data from our users, including:\n",
      "\n",
      "* Account/usage data (~500GB/month)\n",
      "* Email configurations\n",
      "* Dashboard settings\n",
      "* Consent rates (e.g., 80% for marketing emails)\n",
      "\n",
      "To protect your privacy, we implement the following measures:\n",
      "\n",
      "* **Encryption**: We encrypt your data in transit and at rest, using industry-standard encryption protocols.\n",
      "* **Access Controls**: We use role-based permissions to limit access to sensitive data, ensuring that only authorized personnel can view or modify it.\n",
      "* **Data Anonymization**: For GDPR-compliant data processing, we anonymize customer data for analytics purposes (privacy_policy.txt).\n",
      "* **Compliance Certifications**: We maintain compliance with global privacy laws, including GDPR, CCPA, and HIPAA (compliance_certifications.csv).\n",
      "\n",
      "To review our data collection practices in more detail, I recommend checking out the following resources:\n",
      "\n",
      "* Our product user guide (product_user_guide.md)\n",
      "* The DataFlow Solutions Privacy Policy (dataflow.com/privacy)\n",
      "* Our compliance certifications (compliance_certifications.csv)\n",
      "\n",
      "If you have any further questions or concerns about your privacy, please don't hesitate to reach out. We're here to help!\n",
      "\n",
      "ðŸ“š Sources Used:\n",
      "   1. ðŸ“ legal_compliance - security_policies.txt\n",
      "   2. ðŸ“ legal_compliance - privacy_policy.txt\n",
      "   3. ðŸ“ legal_compliance - privacy_policy.txt\n",
      "\n",
      "â±ï¸ Response Time: 57.75 seconds\n",
      "ðŸŽ¯ Department Accuracy: âœ… Accurate\n"
     ]
    }
   ],
   "source": [
    "def test_customer_scenarios(agent):\n",
    "    \"\"\"Test agent with realistic customer service scenarios\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ­ TESTING CUSTOMER SERVICE SCENARIOS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Realistic customer questions\n",
    "    scenarios = [\n",
    "        {\n",
    "            \"question\": \"What are your pricing plans and how much does the premium plan cost?\",\n",
    "            \"category\": \"Billing\",\n",
    "            \"expected_dept\": \"business_data\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How do I authenticate with your API? I'm getting authentication errors.\",\n",
    "            \"category\": \"Technical Support\",\n",
    "            \"expected_dept\": \"customer_facing\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What data do you collect and how do you protect my privacy?\",\n",
    "            \"category\": \"Privacy/Legal\",\n",
    "            \"expected_dept\": \"legal_compliance\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios, 1):\n",
    "        print(f\"\\nðŸ“ž Scenario {i}: {scenario['category']}\")\n",
    "        print(f\"â“ Question: {scenario['question']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get agent response\n",
    "        response = agent.ask(scenario[\"question\"])\n",
    "        \n",
    "        print(f\"ðŸ¤– Agent Response:\")\n",
    "        print(f\"   {response['answer']}\")  # Show complete response\n",
    "        \n",
    "        print(f\"\\nðŸ“š Sources Used:\")\n",
    "        for j, source in enumerate(response['sources'][:3], 1):  # Show top 3 sources\n",
    "            print(f\"   {j}. ðŸ“ {source['department']} - {source['file']}\")\n",
    "        \n",
    "        print(f\"\\nâ±ï¸ Response Time: {response['response_time']:.2f} seconds\")\n",
    "        \n",
    "        # Check if correct department was used\n",
    "        dept_match = any(source['department'] == scenario['expected_dept'] for source in response['sources'])\n",
    "        accuracy = \"âœ… Accurate\" if dept_match else \"âš ï¸ Needs Review\"\n",
    "        print(f\"ðŸŽ¯ Department Accuracy: {accuracy}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"scenario\": scenario,\n",
    "            \"response\": response,\n",
    "            \"accurate\": dept_match\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the scenarios\n",
    "test_results = test_customer_scenarios(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’° BUSINESS IMPACT ANALYSIS\n",
      "==============================\n",
      "ðŸ“Š PERFORMANCE METRICS:\n",
      "   Accuracy Rate: 100.0%\n",
      "   Avg Response Time: 48.77 seconds\n",
      "   Questions Handled: 6\n",
      "\n",
      "ðŸ’µ COST ANALYSIS:\n",
      "   Human Response Time: 300 seconds avg\n",
      "   AI Response Time: 48.8 seconds avg\n",
      "\n",
      "ðŸŽ¯ BUSINESS IMPACT:\n",
      "   Hours Saved Daily: 3.5 hours\n",
      "   Daily Cost Savings: $87.23\n",
      "   Annual Cost Savings: $21,808.01\n"
     ]
    }
   ],
   "source": [
    "def calculate_business_impact(agent, test_results):\n",
    "    \"\"\"Calculate measurable business impact and ROI\"\"\"\n",
    "    \n",
    "    print(\"ðŸ’° BUSINESS IMPACT ANALYSIS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Get agent performance stats\n",
    "    stats = agent.get_stats()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accurate_responses = sum(1 for result in test_results if result['accurate'])\n",
    "    accuracy_rate = (accurate_responses / len(test_results)) * 100 if test_results else 0\n",
    "    \n",
    "    # Business metrics\n",
    "    metrics = {\n",
    "        \"daily_customer_questions\": 50,\n",
    "        \"avg_human_response_time\": 300,  # 5 minutes\n",
    "        \"hourly_support_cost\": 25,\n",
    "        \"working_days_per_year\": 250,\n",
    "        \"ai_accuracy_rate\": accuracy_rate,\n",
    "        \"ai_avg_response_time\": stats.get('avg_response_time', 0)\n",
    "    }\n",
    "    \n",
    "    # Calculate savings\n",
    "    daily_human_hours = (metrics['daily_customer_questions'] * metrics['avg_human_response_time']) / 3600\n",
    "    daily_ai_hours = (metrics['daily_customer_questions'] * metrics['ai_avg_response_time']) / 3600\n",
    "    \n",
    "    hours_saved_daily = daily_human_hours - daily_ai_hours\n",
    "    daily_cost_savings = hours_saved_daily * metrics['hourly_support_cost']\n",
    "    annual_savings = daily_cost_savings * metrics['working_days_per_year']\n",
    "    \n",
    "    print(f\"ðŸ“Š PERFORMANCE METRICS:\")\n",
    "    print(f\"   Accuracy Rate: {accuracy_rate:.1f}%\")\n",
    "    print(f\"   Avg Response Time: {metrics['ai_avg_response_time']:.2f} seconds\")\n",
    "    print(f\"   Questions Handled: {stats.get('conversations', 0)}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’µ COST ANALYSIS:\")\n",
    "    print(f\"   Human Response Time: {metrics['avg_human_response_time']} seconds avg\")\n",
    "    print(f\"   AI Response Time: {metrics['ai_avg_response_time']:.1f} seconds avg\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ BUSINESS IMPACT:\")\n",
    "    print(f\"   Hours Saved Daily: {hours_saved_daily:.1f} hours\")\n",
    "    print(f\"   Daily Cost Savings: ${daily_cost_savings:.2f}\")\n",
    "    print(f\"   Annual Cost Savings: ${annual_savings:,.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy_rate\": accuracy_rate,\n",
    "        \"annual_savings\": annual_savings,\n",
    "        \"hours_saved_daily\": hours_saved_daily\n",
    "    }\n",
    "\n",
    "# Calculate business impact\n",
    "business_impact = calculate_business_impact(agent, test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” FINAL SYSTEM VALIDATION\n",
      "==============================\n",
      "   âœ… Document chunks loaded: 477\n",
      "   âœ… Vector store created\n",
      "   âœ… LLM connected: ollama_modern\n",
      "   âœ… RAG chain built\n",
      "   âœ… Customer service agent ready\n",
      "\n",
      "ðŸ“ˆ PERFORMANCE VALIDATION:\n",
      "   âœ… Accuracy Rate: 100.0%\n",
      "   âœ… Response Time: 48.77s avg\n",
      "   âœ… Business Impact: $21,808 annual savings\n",
      "\n",
      "ðŸŽ‰ SUCCESS! COMPLETE RAG SYSTEM OPERATIONAL\n",
      "ðŸ¤– DataFlow's AI customer service agent is ready for production!\n",
      "â­ EXCELLENT: High accuracy + strong business case\n"
     ]
    }
   ],
   "source": [
    "# Final system validation\n",
    "print(\"ðŸ” FINAL SYSTEM VALIDATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# System components check\n",
    "components = [\n",
    "    (len(chunks) > 0, f\"Document chunks loaded: {len(chunks)}\"),\n",
    "    (vector_store is not None, \"Vector store created\"),\n",
    "    (llm is not None, f\"LLM connected: {llm_type if llm else 'None'}\"),\n",
    "    (rag_chain is not None, \"RAG chain built\"),\n",
    "    (agent is not None, \"Customer service agent ready\")\n",
    "]\n",
    "\n",
    "all_systems_go = True\n",
    "for check, message in components:\n",
    "    status = \"âœ…\" if check else \"âŒ\"\n",
    "    print(f\"   {status} {message}\")\n",
    "    if not check:\n",
    "        all_systems_go = False\n",
    "\n",
    "# Performance validation\n",
    "if test_results:\n",
    "    accuracy = sum(1 for r in test_results if r['accurate']) / len(test_results) * 100\n",
    "    print(f\"\\nðŸ“ˆ PERFORMANCE VALIDATION:\")\n",
    "    print(f\"   âœ… Accuracy Rate: {accuracy:.1f}%\")\n",
    "    print(f\"   âœ… Response Time: {agent.get_stats().get('avg_response_time', 0):.2f}s avg\")\n",
    "    print(f\"   âœ… Business Impact: ${business_impact.get('annual_savings', 0):,.0f} annual savings\")\n",
    "\n",
    "if all_systems_go:\n",
    "    print(\"\\nðŸŽ‰ SUCCESS! COMPLETE RAG SYSTEM OPERATIONAL\")\n",
    "    print(\"ðŸ¤– DataFlow's AI customer service agent is ready for production!\")\n",
    "    \n",
    "    if business_impact.get('accuracy_rate', 0) >= 75:\n",
    "        print(\"â­ EXCELLENT: High accuracy + strong business case\")\n",
    "    elif business_impact.get('accuracy_rate', 0) >= 60:\n",
    "        print(\"ðŸ‘ GOOD: Solid foundation for customer service automation\")\n",
    "    else:\n",
    "        print(\"âš ï¸ NEEDS IMPROVEMENT: Consider fine-tuning\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ PARTIAL SUCCESS: Some components need attention\")\n",
    "    print(\"ðŸ’¡ Check LLM setup (Ollama or OpenAI) for full functionality\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
